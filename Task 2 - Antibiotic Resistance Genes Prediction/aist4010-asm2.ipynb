{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:00.580060Z","iopub.execute_input":"2022-03-17T04:25:00.580362Z","iopub.status.idle":"2022-03-17T04:25:01.339233Z","shell.execute_reply.started":"2022-03-17T04:25:00.580261Z","shell.execute_reply":"2022-03-17T04:25:01.338403Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Mar 17 04:25:01 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   38C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport re\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom Bio import SeqIO\nfrom matplotlib import pyplot as plt\nfrom sklearn import metrics\nfrom torch.utils.data import Dataset\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-03-17T04:25:01.341418Z","iopub.execute_input":"2022-03-17T04:25:01.341685Z","iopub.status.idle":"2022-03-17T04:25:08.454536Z","shell.execute_reply.started":"2022-03-17T04:25:01.341646Z","shell.execute_reply":"2022-03-17T04:25:08.453846Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# setup seed for experiment reproducibility\ndef setup_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nsetup_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:08.455696Z","iopub.execute_input":"2022-03-17T04:25:08.456646Z","iopub.status.idle":"2022-03-17T04:25:08.464455Z","shell.execute_reply.started":"2022-03-17T04:25:08.456613Z","shell.execute_reply":"2022-03-17T04:25:08.463709Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# protein dataset\nclass ProteinDataset(Dataset):\n    def __init__(self, split=\"train\", tokenizer_name='Rostlab/prot_bert_bfd', max_length=1024):\n        self.max_length = max_length\n        \n        # define tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n        \n        # define label mapping\n        self.arg_dict = {\"aminoglycoside\": 0, \"macrolide-lincosamide-streptogramin\": 1, \n                         \"polymyxin\": 2, \"fosfomycin\": 3, \"trimethoprim\": 4, \"bacitracin\": 5, \n                         \"quinolone\": 6, \"multidrug\": 7, \"chloramphenicol\": 8, \n                         \"tetracycline\": 9, \"rifampin\": 10, \"beta_lactam\": 11,\n                         \"sulfonamide\": 12, \"glycopeptide\": 13, \"nonarg\": 14}\n        self.labels_dic = {id: tag for tag, id in self.arg_dict.items()}\n        \n        # define folder paths\n        self.datasetFolderPath = '/kaggle/input/aist4010-a2/data/'\n        self.trainFilePath = os.path.join(self.datasetFolderPath, 'train.fasta')\n        self.valFilePath = os.path.join(self.datasetFolderPath, 'val.fasta')\n        self.testFilePath = os.path.join(self.datasetFolderPath, 'test.fasta')\n        \n        # load dataset from suitable file\n        if split == \"train\":\n            self.seqs, self.labels = self.load_dataset(self.trainFilePath, max_length)\n        elif split == \"val\":\n            self.seqs, self.labels = self.load_dataset(self.valFilePath, max_length)\n        else:\n            self.seqs, self.labels = self.load_dataset(self.testFilePath, max_length, True)\n    \n    # load dataset in 'FASTA' format\n    def load_dataset(self, path, max_len, test=False):\n        strs = []\n        labels = []\n        count = 0\n        for record in SeqIO.parse(path, \"fasta\"):\n            count += 1\n            # get the protein sequence\n            x = str(record.seq)\n            # extract label from description, add dummy label for test data\n            tmp = record.id.split(\"|\")\n            y = 14 if (test or tmp[0] == \"sp\") else self.arg_dict[tmp[3]]\n            strs.append(x)\n            labels.append(y)\n        if (test):\n            return strs, labels\n        else:\n            # shuffle the training and validation dataset\n            shuffle = random.sample(list(range(count)), count)\n            return [strs[i] for i in shuffle], [labels[i] for i in shuffle]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        # parse the sequence to proper format\n        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n        # substitute rare amino acid by 'X'\n        seq = re.sub(r\"[UZOB]\", \"X\", seq)\n        # preprocess sequence with tokenizer\n        seq_ids = self.tokenizer(seq, truncation=True, padding='max_length', max_length=self.max_length)\n        sample = {key: torch.tensor(val) for key, val in seq_ids.items()}\n        sample['labels'] = torch.tensor(self.labels[idx])\n        return sample","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:08.467015Z","iopub.execute_input":"2022-03-17T04:25:08.467417Z","iopub.status.idle":"2022-03-17T04:25:08.486001Z","shell.execute_reply.started":"2022-03-17T04:25:08.467380Z","shell.execute_reply":"2022-03-17T04:25:08.485063Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# pre-trained ProtBert-BFD model\nmodel_name = 'Rostlab/prot_bert_bfd'\n\n# load the datasets\ntrain_dataset = ProteinDataset(split=\"train\", tokenizer_name=model_name, max_length=400)\nval_dataset = ProteinDataset(split=\"val\", tokenizer_name=model_name, max_length=400)\ntest_dataset = ProteinDataset(split=\"test\", tokenizer_name=model_name, max_length=400)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:08.487231Z","iopub.execute_input":"2022-03-17T04:25:08.488067Z","iopub.status.idle":"2022-03-17T04:25:24.013220Z","shell.execute_reply.started":"2022-03-17T04:25:08.488017Z","shell.execute_reply":"2022-03-17T04:25:24.012455Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/86.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b52bb516850478d8b5c9767b2a11f31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/361 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"407b44fc214f4de68f6ff822e3d1a25d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/81.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c1e603266146ac9367213be8ca5ed7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08047dabfd094de594bfef87e88b1072"}},"metadata":{}}]},{"cell_type":"code","source":"# compute different metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = metrics.accuracy_score(labels, preds)\n    precision, recall, f1, _ = metrics.precision_recall_fscore_support(labels, preds, average='macro')\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': precision,\n        'recall': recall\n    }","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:24.014744Z","iopub.execute_input":"2022-03-17T04:25:24.015016Z","iopub.status.idle":"2022-03-17T04:25:24.021320Z","shell.execute_reply.started":"2022-03-17T04:25:24.014979Z","shell.execute_reply":"2022-03-17T04:25:24.019744Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# initialize the model\ndef model_init():\n    return AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=15)","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:24.022660Z","iopub.execute_input":"2022-03-17T04:25:24.023203Z","iopub.status.idle":"2022-03-17T04:25:24.033610Z","shell.execute_reply.started":"2022-03-17T04:25:24.023164Z","shell.execute_reply":"2022-03-17T04:25:24.032723Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# define training argument\ntraining_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=5,              # total number of training epochs\n    per_device_train_batch_size=1,   # batch size per device during training\n    per_device_eval_batch_size=32,   # batch size for evaluation\n    learning_rate=5e-5,              # learning rate\n    lr_scheduler_type=\"linear\",      # learning rate decay\n    warmup_steps=0,                  # number of warmup steps for learning rate scheduler\n    weight_decay=5e-3,               # strength of weight decay\n    logging_strategy=\"epoch\",        # log after each epoch\n    save_strategy=\"no\",              # do not save in the middle of training\n    do_train=True,                   # perform training\n    do_eval=True,                    # perform evaluation\n    evaluation_strategy=\"epoch\",     # evalute after each epoch\n    gradient_accumulation_steps=64,  # total number of steps before back propagation\n    fp16=True,                       # use mixed precision\n    fp16_opt_level=\"02\",             # mixed precision mode\n    report_to=\"none\",                # no integrations for result report\n    run_name=\"ProBert-BFD-MS\",       # experiment name\n    seed=42                          # seed for experiment reproducibility\n)\n\n# initialize trainer\ntrainer = Trainer(\n    model_init=model_init,                # the instantiated Transformers model to be trained\n    args=training_args,                   # training arguments, defined above\n    train_dataset=train_dataset,          # training dataset\n    eval_dataset=val_dataset,             # evaluation dataset\n    compute_metrics = compute_metrics,    # evaluation metrics\n)\n\n# fine-tuning the pre-trained model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-17T04:25:24.035015Z","iopub.execute_input":"2022-03-17T04:25:24.035463Z","iopub.status.idle":"2022-03-17T11:39:40.793697Z","shell.execute_reply.started":"2022-03-17T04:25:24.035424Z","shell.execute_reply":"2022-03-17T11:39:40.793012Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"loading configuration file https://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/67f460acc7e7e147ff828e909ffe419d00d66ce679c682bc4ab715c107bcbe41.baf557855a8618d0ddfb6c23bfd135bfc38ccf8c3fb099b8df45eb110ccf05e9\nModel config BertConfig {\n  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\",\n    \"10\": \"LABEL_10\",\n    \"11\": \"LABEL_11\",\n    \"12\": \"LABEL_12\",\n    \"13\": \"LABEL_13\",\n    \"14\": \"LABEL_14\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_10\": 10,\n    \"LABEL_11\": 11,\n    \"LABEL_12\": 12,\n    \"LABEL_13\": 13,\n    \"LABEL_14\": 14,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 40000,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 30,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.15.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30\n}\n\nhttps://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnt2p6rxx\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0708bfb2aa24f2dbcdf3baa92672ea0"}},"metadata":{}},{"name":"stderr","text":"storing https://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0a05878f9e3a0d39834dc6f21b88471696d7453a07bac7246152a6ef307c9af4.c5b9869da882baaf70e8e70cf32d81500803511e3220e24457115a03445fa65f\ncreating metadata file for /root/.cache/huggingface/transformers/0a05878f9e3a0d39834dc6f21b88471696d7453a07bac7246152a6ef307c9af4.c5b9869da882baaf70e8e70cf32d81500803511e3220e24457115a03445fa65f\nloading weights file https://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a05878f9e3a0d39834dc6f21b88471696d7453a07bac7246152a6ef307c9af4.c5b9869da882baaf70e8e70cf32d81500803511e3220e24457115a03445fa65f\nSome weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing amp half precision backend\nloading configuration file https://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/67f460acc7e7e147ff828e909ffe419d00d66ce679c682bc4ab715c107bcbe41.baf557855a8618d0ddfb6c23bfd135bfc38ccf8c3fb099b8df45eb110ccf05e9\nModel config BertConfig {\n  \"_name_or_path\": \"Rostlab/prot_bert_bfd\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.0,\n  \"classifier_dropout\": null,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.0,\n  \"hidden_size\": 1024,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\",\n    \"3\": \"LABEL_3\",\n    \"4\": \"LABEL_4\",\n    \"5\": \"LABEL_5\",\n    \"6\": \"LABEL_6\",\n    \"7\": \"LABEL_7\",\n    \"8\": \"LABEL_8\",\n    \"9\": \"LABEL_9\",\n    \"10\": \"LABEL_10\",\n    \"11\": \"LABEL_11\",\n    \"12\": \"LABEL_12\",\n    \"13\": \"LABEL_13\",\n    \"14\": \"LABEL_14\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_10\": 10,\n    \"LABEL_11\": 11,\n    \"LABEL_12\": 12,\n    \"LABEL_13\": 13,\n    \"LABEL_14\": 14,\n    \"LABEL_2\": 2,\n    \"LABEL_3\": 3,\n    \"LABEL_4\": 4,\n    \"LABEL_5\": 5,\n    \"LABEL_6\": 6,\n    \"LABEL_7\": 7,\n    \"LABEL_8\": 8,\n    \"LABEL_9\": 9\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 40000,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 30,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"transformers_version\": \"4.15.0\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 30\n}\n\nloading weights file https://huggingface.co/Rostlab/prot_bert_bfd/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0a05878f9e3a0d39834dc6f21b88471696d7453a07bac7246152a6ef307c9af4.c5b9869da882baaf70e8e70cf32d81500803511e3220e24457115a03445fa65f\nSome weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n***** Running training *****\n  Num examples = 21209\n  Num Epochs = 5\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 64\n  Gradient Accumulation steps = 64\n  Total optimization steps = 1655\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1655' max='1655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1655/1655 7:13:03, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.492200</td>\n      <td>0.165568</td>\n      <td>0.967632</td>\n      <td>0.705353</td>\n      <td>0.711810</td>\n      <td>0.704908</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.111500</td>\n      <td>0.103859</td>\n      <td>0.981536</td>\n      <td>0.871949</td>\n      <td>0.879590</td>\n      <td>0.871615</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.058500</td>\n      <td>0.079554</td>\n      <td>0.988375</td>\n      <td>0.959785</td>\n      <td>0.973369</td>\n      <td>0.948042</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.034300</td>\n      <td>0.068231</td>\n      <td>0.989742</td>\n      <td>0.967727</td>\n      <td>0.974476</td>\n      <td>0.962261</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.022400</td>\n      <td>0.066807</td>\n      <td>0.989742</td>\n      <td>0.968221</td>\n      <td>0.975802</td>\n      <td>0.961898</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1373: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\n***** Running Evaluation *****\n  Num examples = 4387\n  Batch size = 32\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.7/site-packages/transformers/trainer.py:1373: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n  args.max_grad_norm,\n***** Running Evaluation *****\n  Num examples = 4387\n  Batch size = 32\n/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n***** Running Evaluation *****\n  Num examples = 4387\n  Batch size = 32\n***** Running Evaluation *****\n  Num examples = 4387\n  Batch size = 32\n***** Running Evaluation *****\n  Num examples = 4387\n  Batch size = 32\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1655, training_loss=0.14377552994788592, metrics={'train_runtime': 25999.0198, 'train_samples_per_second': 4.079, 'train_steps_per_second': 0.064, 'total_flos': 9.6424021998864e+16, 'train_loss': 0.14377552994788592, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"# make predictions using the trained model\npredictions, label_ids, metrics = trainer.predict(test_dataset)\npredictions_max = np.argmax(predictions, axis=1)\n\n# output the predictions to the csv file\noutput = {\"id\": np.array([\"SEQ\" + str(i) for i in range(len(predictions_max))]), \"label\": np.array(predictions_max)}\noutput_df = pd.DataFrame(output).set_index('id')\noutput_df.to_csv(\"output.csv\")\n\n# save the trained model\ntrainer.save_model('./models/')","metadata":{"execution":{"iopub.status.busy":"2022-03-17T11:39:40.797146Z","iopub.execute_input":"2022-03-17T11:39:40.797627Z","iopub.status.idle":"2022-03-17T11:44:12.541978Z","shell.execute_reply.started":"2022-03-17T11:39:40.797587Z","shell.execute_reply":"2022-03-17T11:44:12.541256Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"***** Running Prediction *****\n  Num examples = 4469\n  Batch size = 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 04:26]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\nSaving model checkpoint to ./models/\nConfiguration saved in ./models/config.json\nModel weights saved in ./models/pytorch_model.bin\n","output_type":"stream"}]}]}